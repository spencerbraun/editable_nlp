{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "considerable-bolivia",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1><span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "floppy-testing",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import random\n",
    "import time\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import spacy\n",
    "from datasets import load_dataset, list_metrics, load_metric\n",
    "# from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "\n",
    "\n",
    "\n",
    "# wikibio = load_dataset('wiki_bio', cache_dir=\"/Volumes/External HD/Dev/datasets/wikibio\", split='train[:1%]')\n",
    "# wikibio[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "billion-layer",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset wikitext (/Volumes/External HD/Dev/datasets/wikitext/wikitext/wikitext-103-raw-v1/1.0.0/47c57a6745aa5ce8e16a5355aaa4039e3aa90d1adad87cef1ad4e0f29e74ac91)\n"
     ]
    }
   ],
   "source": [
    "wikitext = load_dataset(\n",
    "    'wikitext', \n",
    "    'wikitext-103-raw-v1', \n",
    "    cache_dir=\"/Volumes/External HD/Dev/datasets/wikitext\", \n",
    "    split='train[:1%]'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "convenient-cooperation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<spacy.pipeline.sentencizer.Sentencizer at 0x135fabf00>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp = spacy.load(\n",
    "    \"en_core_web_sm\", \n",
    "    exclude=['tagger', 'parser', 'attribute_ruler', 'lemmatizer']\n",
    ") #Note: use larger model for production\n",
    "nlp.add_pipe('sentencizer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aware-spain",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cognitive-envelope",
   "metadata": {},
   "source": [
    "When processing large volumes of text, the statistical models are usually more efficient if you let them work on batches of texts. spaCyâ€™s nlp.pipe method takes an iterable of texts and yields processed Doc objects. The batching is done internally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "smoking-scientist",
   "metadata": {},
   "outputs": [],
   "source": [
    "def runNER(model, texts):\n",
    "    for doc in nlp.pipe(texts):\n",
    "        for sent in doc.sents:\n",
    "            if sent.ents:\n",
    "                ents = str([(e.text, e.start_char - sent.start_char, e.end_char - sent.start_char, e.label_) for e in sent.ents]).strip('[]')\n",
    "                out = f\"{sent.text}|{ents}\"\n",
    "                print(out)\n",
    "                print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "appropriate-companion",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataProcessor:\n",
    "    def __init__(\n",
    "        self, \n",
    "        text, \n",
    "        write_dir=None, \n",
    "        parallel=False\n",
    "    ):\n",
    "        self.text = text\n",
    "        self.write_dir = write_dir\n",
    "        self.parallel = parallel\n",
    "        \n",
    "        self.ner_texts = []\n",
    "        self.ents = collections.defaultdict(list)\n",
    "        self.permuted = []\n",
    "\n",
    "        self.model = spacy.load(\n",
    "            \"en_core_web_sm\", \n",
    "            exclude=['tagger', 'parser', 'attribute_ruler', 'lemmatizer']\n",
    "        )\n",
    "        self.model.add_pipe('sentencizer')\n",
    "        \n",
    "        self.keep_ents = ['PERSON', 'ORG', 'GPE']\n",
    "        \n",
    "    @classmethod\n",
    "    def fromFile(cls, file_loc):\n",
    "        pass\n",
    "    \n",
    "    \n",
    "    def run(self, func, args):\n",
    "        if self.parallel:\n",
    "            with ProcessPoolExecutor() as executor:\n",
    "                for output in executor.map(func, args):\n",
    "                    return output.result(timeout=None)\n",
    "        else:\n",
    "            for output in func(*args):\n",
    "                yield output\n",
    "            \n",
    "    \n",
    "    \n",
    "    def permuteEnts(self):\n",
    "        timestamp = time.time()\n",
    "        if self.write_dir:\n",
    "            handler = open(self.write_dir + f'/permuted_entities.{timestamp}', 'w')\n",
    "        for (sent, ents) in self.ner_texts:\n",
    "            eligible = list(filter(lambda x: x[3] in self.keep_ents, ents))\n",
    "            orig_ent = random.choice(eligible)\n",
    "            ent_type = orig_ent[3]\n",
    "            start, end  = orig_ent[1:3]\n",
    "            while True:\n",
    "                replace_ent = random.choice(self.ents[ent_type])\n",
    "                if replace_ent != orig_ent: break\n",
    "\n",
    "            prefix = sent[:start]\n",
    "            suffix = sent[end:]\n",
    "            new_sent = prefix + replace_ent + suffix\n",
    "            if self.write_dir:\n",
    "                handler.write(new_sent + \"\\n\")\n",
    "            self.permuted.append(new_sent)\n",
    "            \n",
    "        if self.write_dir:\n",
    "            handler.close()\n",
    "    \n",
    "    \n",
    "    def processEnts(self):\n",
    "        timestamp = time.time()\n",
    "        \n",
    "        if self.write_dir:\n",
    "            main_loc = self.write_dir + f'/processed_entities.{timestamp}'\n",
    "            ent_loc = self.write_dir + f'/all_entities.{timestamp}'\n",
    "            with open(main_loc, 'w') as f:\n",
    "                for output in self.runNER(self.text):\n",
    "                    text, ents = output\n",
    "                    f.write(f\"{text}|{str(ents).strip('[]')}\\n\")\n",
    "                    self.ner_texts.append(output)\n",
    "            with open(ent_loc, 'w') as f:\n",
    "                for ent_type, ent_value in self.ents.items():\n",
    "                    f.write(f\"{ent_type}|{ent_value}\\n\")\n",
    "        else:\n",
    "            for output in self.runNER(self.text):\n",
    "                self.ner_texts.append(output)\n",
    "        \n",
    "        \n",
    "    def runNER(self, texts):\n",
    "        for doc in self.model.pipe(texts):\n",
    "            for sent in doc.sents:\n",
    "                if any([e.label_ in self.keep_ents for e in sent.ents]):\n",
    "                    ents = []\n",
    "                    for e in sent.ents:\n",
    "                        ents.append((e.text, e.start_char - sent.start_char, e.end_char - sent.start_char, e.label_))\n",
    "                        self.ents[e.label_].append(e.text)\n",
    "                    yield (sent.text, ents)\n",
    "    \n",
    "    def __repr__(self):\n",
    "        \n",
    "        return (f\"DataProcessor:<{len(self.text)} RAW>\"\n",
    "                f\"<{len(self.ner_texts)} NER>\"\n",
    "                f\"<{len(self.permuted)} PERM>\"\n",
    "                f\"<{sum([len(self.ents[k]) for k in self.ents])} ENTS>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "north-alexandria",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def is_ascii(s):\n",
    "    return all(ord(c) < 128 for c in s)\n",
    "\n",
    "def filterText(iterator):\n",
    "\n",
    "    valid  = []\n",
    "    for text in iterator:\n",
    "        if len(text) < 50:\n",
    "            continue\n",
    "        if not is_ascii(text):\n",
    "            continue\n",
    "        valid.append(text)\n",
    "\n",
    "    return valid\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "encouraging-shipping",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampleText = filterText(wikitext['text'][1:10000])\n",
    "\n",
    "dp = DataProcessor(sampleText, write_dir='./data')\n",
    "dp.keep_ents = ['PERSON']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "bibliographic-cornell",
   "metadata": {},
   "outputs": [],
   "source": [
    "dp.processEnts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "surgical-extreme",
   "metadata": {},
   "outputs": [],
   "source": [
    "dp.permuteEnts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "endless-morrison",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataProcessor:<3160 RAW><4343 NER><4343 PERM><15773 ENTS>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "impossible-relaxation",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "abroad-cancellation",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataProcessor:\n",
    "    def __init__(\n",
    "        self, \n",
    "        text, \n",
    "        write_dir=None, \n",
    "        parallel=False\n",
    "    ):\n",
    "        self.text = text\n",
    "\n",
    "        self.write_dir = write_dir\n",
    "        self.parallel = parallel\n",
    "        \n",
    "        self.raw_texts = []\n",
    "        self.ner_texts = []\n",
    "        self.permuted = []\n",
    "        self.changed_ents = []\n",
    "        \n",
    "        self.ents = collections.defaultdict(list)\n",
    "\n",
    "        self.model = spacy.load(\n",
    "            \"en_core_web_sm\", \n",
    "            exclude=['tagger', 'parser', 'attribute_ruler', 'lemmatizer']\n",
    "        )\n",
    "        self.model.add_pipe('sentencizer')\n",
    "        \n",
    "        self.keep_ents = ['PERSON', 'ORG', 'GPE']\n",
    "        \n",
    "    @classmethod\n",
    "    def fromFile(cls, file_loc):\n",
    "        pass\n",
    "    \n",
    "    \n",
    "    def run(self, func, args):\n",
    "        if self.parallel:\n",
    "            with ProcessPoolExecutor() as executor:\n",
    "                for output in executor.map(func, args):\n",
    "                    return output.result(timeout=None)\n",
    "        else:\n",
    "            for output in func(*args):\n",
    "                yield output\n",
    "            \n",
    "    \n",
    "    \n",
    "    def permuteEnts(self):\n",
    "        timestamp = time.time()\n",
    "        \n",
    "        if self.write_dir:\n",
    "            permuteFile = open(self.write_dir + f'/permuted_entities.{timestamp}', 'w')\n",
    "            origFile = open(self.write_dir + f'/original_entities.{timestamp}', 'w')\n",
    "            entFile = open(self.write_dir + f'/entity_swaps.{timestamp}', 'w')\n",
    "            \n",
    "        for idx, (sent, ents) in enumerate(self.ner_texts):\n",
    "            eligible = list(filter(lambda x: x[3] in self.keep_ents, ents))\n",
    "            orig_ent = random.choice(eligible)\n",
    "            ent_type = orig_ent[3]\n",
    "            start, end  = orig_ent[1:3]\n",
    "            while True:\n",
    "                replace_ent = random.choice(self.ents[ent_type])\n",
    "                if replace_ent != orig_ent[0]: break\n",
    "\n",
    "            prefix = sent[:start]\n",
    "            suffix = sent[end:]\n",
    "            new_sent = prefix + replace_ent + suffix\n",
    "\n",
    "            if self.write_dir:\n",
    "                permuteFile.write(new_sent + \"\\n\")\n",
    "                origFile.write(self.raw_texts[idx].strip('\\n').strip(\" \") + \"\\n\")\n",
    "                entFile.write(f\"{orig_ent[0]}|{replace_ent}\\n\")\n",
    "                \n",
    "            self.permuted.append(new_sent)\n",
    "            self.changed_ents.append((orig_ent[0], replace_ent))\n",
    "            \n",
    "        if self.write_dir:\n",
    "            permuteFile.close()\n",
    "            origFile.close()\n",
    "            entFile.close()\n",
    "    \n",
    "    \n",
    "    def processEnts(self):\n",
    "                \n",
    "        for output in self.runNER(self.text):\n",
    "            self.ner_texts.append(output)\n",
    "        \n",
    "        \n",
    "    def runNER(self, texts):\n",
    "        for doc in self.model.pipe(texts):\n",
    "            processed = []\n",
    "            for sent in doc.sents:\n",
    "                if any([e.label_ in self.keep_ents for e in sent.ents]):\n",
    "                    ents = []\n",
    "                    for e in sent.ents:\n",
    "                        ents.append((e.text, e.start_char - sent.start_char, e.end_char - sent.start_char, e.label_))\n",
    "                        self.ents[e.label_].append(e.text)\n",
    "                    processed.append((sent.text, ents))\n",
    "            if processed:\n",
    "                self.raw_texts.append(doc.text)\n",
    "                yield random.choice(processed)\n",
    "            \n",
    "    \n",
    "    def __repr__(self):\n",
    "        \n",
    "        return (f\"DataProcessor:<{len(self.text)} RAW>\"\n",
    "                f\"<{len(self.ner_texts)} NER>\"\n",
    "                f\"<{len(self.permuted)} PERM>\"\n",
    "                f\"<{sum([len(self.ents[k]) for k in self.ents])} ENTS>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "activated-greeting",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampleText = filterText(wikitext['text'][1:1000])\n",
    "\n",
    "dp = DataProcessor(sampleText, write_dir='./data')\n",
    "dp.keep_ents = ['PERSON']\n",
    "dp.processEnts()\n",
    "dp.permuteEnts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "surrounded-gnome",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataProcessor:<275 RAW><160 NER><160 PERM><1095 ENTS>"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "outstanding-psychology",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' The game began development in 2010 , carrying over a large portion of the work done on Dissident Aggressor .'"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dp.permuted[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "acting-miracle",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" The game began development in 2010 , carrying over a large portion of the work done on Valkyria Chronicles II . While it retained the standard features of the series , it also underwent multiple adjustments , such as making the game more forgiving for series newcomers . Character designer Raita Honjou and composer Hitoshi Sakimoto both returned from previous entries , along with Valkyria Chronicles II director Takeshi Ozawa . A large team of writers handled the script . The game 's opening theme was sung by May 'n . \\n\""
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dp.raw_texts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "another-conspiracy",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Valkyria Chronicles II', 'Dissident Aggressor')"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dp.changed_ents[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "parliamentary-render",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "altered-nothing",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
