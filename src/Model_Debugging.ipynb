{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "australian-population",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import higher\n",
    "\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "\n",
    "import utils"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beautiful-recovery",
   "metadata": {},
   "source": [
    "## Load Model, Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "moved-wagon",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "correct-orchestra",
   "metadata": {},
   "source": [
    "Try using GPT2 vs distilgpt2 (KL div actually goes up though)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "junior-merchandise",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadOTSModel():\n",
    "    model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "three-notion",
   "metadata": {},
   "outputs": [],
   "source": [
    "model, tokenizer = loadOTSModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "exotic-chile",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = utils.retrieveDataloader(\n",
    "        tokenizer, \n",
    "        bs=1, \n",
    "        dataset='train'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "coupled-candy",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20892"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "approved-leadership",
   "metadata": {},
   "outputs": [],
   "source": [
    "for train_step, (lm_data, edit_example, _) in enumerate(dataloader):\n",
    "\n",
    "    lm_tokens, lm_mask = lm_data\n",
    "    lm_tokens, lm_mask = lm_tokens.to(device), lm_mask.to(device)\n",
    "    edit_tokens, edit_mask = edit_example\n",
    "    edit_tokens, edit_mask = edit_tokens.to(device), edit_mask.to(device)\n",
    "\n",
    "    lm_labels = lm_tokens.masked_fill(lm_mask == 0, -100)\n",
    "    edit_labels = edit_tokens.masked_fill(edit_mask == 0, -100) \n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "personalized-dispatch",
   "metadata": {},
   "source": [
    "## Double Check Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "premium-bernard",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[14291,   465, 12928,   422, 29032,  3841,   837, 18322,  4488,   329,\n",
       "         39964,  1578,   764,  2102,   837,  1708,   281,  5095,   287,   662,\n",
       "          2488,    12,    31,  1622,   837, 18322,  1043,  6443,  3614,   837,\n",
       "           878,  4191,  1642,  1478,  4652, 11057,   764,   679,  4191,  1364,\n",
       "           262,  3430,  1863,   351,  5891, 26318,  4186,  6706,   626,   735,\n",
       "           287,  3269,  1853,   764,   198, 50256, 50256, 50256, 50256, 50256,\n",
       "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "spare-taste",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0]], device='cuda:0')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "blessed-punch",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[14291,   465, 12928,   422, 29032,  3841,   837, 18322,  4488,   329,\n",
       "         39964,  1578,   764,  2102,   837,  1708,   281,  5095,   287,   662,\n",
       "          2488,    12,    31,  1622,   837, 18322,  1043,  6443,  3614,   837,\n",
       "           878,  4191,  1642,  1478,  4652, 11057,   764,   679,  4191,  1364,\n",
       "           262,  3430,  1863,   351,  5891, 26318,  4186,  6706,   626,   735,\n",
       "           287,  3269,  1853,   764,   198,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "stable-calendar",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Following his departure from Dartford, Julian signed for Sutton United. However, following an injury in pre @-@ season, Julian found opportunities limited, before eventually making 14 league appearances. He eventually left the club along with fellow goalkeeper Tom Lovelock in January 2015.\\n'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(lm_tokens[lm_labels != -100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "angry-prior",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Following his departure from Dartford, Julian signed for Sutton United. However, following an injury in pre @-@ season, Julian found opportunities limited, before eventually making 14 league appearances. He eventually left the club along with fellow goalkeeper Tom Lovelock in January 2015.\\n'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(lm_labels[lm_labels != -100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "occupied-senior",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(lm_tokens[lm_labels != -100]) == tokenizer.decode(lm_labels[lm_labels != -100])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "violent-orleans",
   "metadata": {},
   "source": [
    "## KL Divergence on same model, same data, model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "commercial-double",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train()\n",
    "model.to(device)\n",
    "model_out1 = model(lm_tokens, attention_mask=lm_mask, labels=lm_labels)\n",
    "model_out2 = model(lm_tokens, attention_mask=lm_mask, labels=lm_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "marked-spending",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(4.7208, device='cuda:0', grad_fn=<NllLossBackward>)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_out1.loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "adjustable-money",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(4.8962, device='cuda:0', grad_fn=<NllLossBackward>)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_out2.loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "useful-blake",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ -83.5357,  -81.1782,  -84.8859,  ...,  -87.5771,  -90.1763,\n",
       "           -82.9961],\n",
       "         [ -82.6794,  -80.3046,  -84.1308,  ...,  -82.5107,  -85.8973,\n",
       "           -81.8964],\n",
       "         [ -69.7387,  -69.7368,  -75.8238,  ...,  -77.5415,  -78.3443,\n",
       "           -71.2514],\n",
       "         ...,\n",
       "         [ -94.0070,  -87.0880,  -89.3705,  ..., -108.8873, -109.0895,\n",
       "           -94.7492],\n",
       "         [ -89.5154,  -82.6523,  -84.5890,  ..., -102.4380, -102.1378,\n",
       "           -89.7528],\n",
       "         [ -97.9364,  -91.3140,  -93.8039,  ..., -111.7013, -111.7482,\n",
       "           -97.7645]]], device='cuda:0', grad_fn=<UnsafeViewBackward>)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_out1.logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "several-mouse",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 13.1518,  13.4113,  13.2810,  ...,  12.9238,  13.3144,  11.1464],\n",
       "         [ 22.9788,  22.9977,  24.8985,  ...,  23.4227,  22.8717,  22.8075],\n",
       "         [  4.4584,   4.0121,   3.3618,  ...,   3.2623,   2.7055,   3.0508],\n",
       "         ...,\n",
       "         [  9.9888,  11.1706,  10.8695,  ...,   7.3800,   7.8151,   8.8785],\n",
       "         [  7.8818,   7.3659,   7.7996,  ...,   7.7815,   7.9289,   8.0477],\n",
       "         [-14.1567, -13.9872, -14.4588,  ..., -12.6759, -11.4402, -17.9914]]],\n",
       "       device='cuda:0', grad_fn=<SubBackward0>)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_out1.logits - model_out2.logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "previous-nowhere",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 200, 50257])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_out1.logits.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "representative-silly",
   "metadata": {},
   "source": [
    "This seems like the wrong dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "legendary-complexity",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1.,  ..., 1., 1., 1.]], device='cuda:0',\n",
       "       grad_fn=<SumBackward1>)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum(F.softmax(model_out1.logits, dim=1), dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "explicit-intelligence",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 50257])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum(F.softmax(model_out1.logits, dim=1), dim=1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "eleven-expert",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(4.4246e-14, device='cuda:0', grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.kl_div(\n",
    "    F.log_softmax(model_out1.logits, dim=1),\n",
    "    F.log_softmax(model_out2.logits, dim=1),\n",
    "    reduction='batchmean',\n",
    "    log_target=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unknown-peoples",
   "metadata": {},
   "source": [
    "This seems like the right dimension - softmax over the 50257 words in vocab for each position in sequence 1-200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "overall-pledge",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000]], device='cuda:0', grad_fn=<SumBackward1>)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum(F.softmax(model_out1.logits, dim=-1), dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "third-miller",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 200])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum(F.softmax(model_out1.logits, dim=-1), dim=-1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "divided-johnson",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(73.3443, device='cuda:0', grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.kl_div(\n",
    "    F.log_softmax(model_out1.logits, dim=-1),\n",
    "    F.log_softmax(model_out2.logits, dim=-1),\n",
    "    reduction='batchmean',\n",
    "    log_target=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "prompt-intermediate",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(73.3443, device='cuda:0', grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kl_loss = nn.KLDivLoss(reduction = 'batchmean')\n",
    "l_loc = kl_loss(\n",
    "    F.log_softmax(model_out1.logits, dim=-1),\n",
    "    F.softmax(model_out2.logits, dim=-1)\n",
    ")\n",
    "l_loc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "orange-spank",
   "metadata": {},
   "source": [
    "## KL Divergence on same model, same data, model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "breeding-ghost",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0., device='cuda:0', grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()\n",
    "model_eval1 = model(lm_tokens, attention_mask=lm_mask, labels=lm_labels)\n",
    "model_eval2 = model(lm_tokens, attention_mask=lm_mask, labels=lm_labels)\n",
    "F.kl_div(\n",
    "    F.log_softmax(model_eval1.logits, dim=-1),\n",
    "    F.log_softmax(model_eval2.logits, dim=-1),\n",
    "    reduction='batchmean',\n",
    "    log_target=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reduced-charlotte",
   "metadata": {},
   "source": [
    "Large KL divergence coming from dropout?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cross-washer",
   "metadata": {},
   "source": [
    "## From editable code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "knowing-clothing",
   "metadata": {},
   "outputs": [],
   "source": [
    "def edit(self, inputs, targets, max_steps=None, model_kwargs=None, loss_kwargs=None, opt_kwargs=None, **kwargs):\n",
    "    \"\"\"\n",
    "    Attempts to edit model (out-of-place) and return an edited copy\n",
    "    :param inputs: data that is fed into the model\n",
    "    :param targets: reference answers that are fed into loss function\n",
    "    :param max_steps: after this many gradient steps the process is terminated\n",
    "    :param model_kwargs: optional extra model inputs, used as model(inputs, **model_params)\n",
    "    :param loss_kwargs: optional extra loss parameters, self.loss_function(model(inputs), targets, **loss_params)\n",
    "    :param opt_kwargs: optional overrides for optimizer.get_initial_state\n",
    "    :param kwargs: extra parameters passed to optimizer.step\n",
    "    :returns: edited_model, is_edit_successful, final_loss, gradients_steps\n",
    "    :rtype: Editable.EditResult\n",
    "    \"\"\"\n",
    "    model_kwargs, loss_kwargs, opt_kwargs = model_kwargs or {}, loss_kwargs or {}, opt_kwargs or {}\n",
    "    optimizer_state = self.optimizer.get_initial_state(self, **opt_kwargs)\n",
    "    editable = self\n",
    "\n",
    "    for step in count():\n",
    "        prediction = editable(inputs, **model_kwargs)\n",
    "        loss = self.loss_function(prediction, targets, **loss_kwargs)\n",
    "\n",
    "        if self.is_edit_finished(**locals()):\n",
    "            return self.EditResult(editable, success=True, loss=loss, complexity=step)\n",
    "        elif step >= (max_steps or self.max_steps):\n",
    "            return self.EditResult(editable, success=False, loss=loss, complexity=step)\n",
    "\n",
    "        optimizer_state, editable = self.optimizer.step(\n",
    "            optimizer_state, editable, loss, parameters=editable.get_editable_parameters(editable.module), **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "brave-northern",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_on_batch(self, x_batch, y_batch, x_edit, y_edit, prefix='train/', is_train=True, **kwargs):\n",
    "    \"\"\" Performs a single gradient update and reports metrics \"\"\"\n",
    "    x_batch, y_batch = map(torch.as_tensor, (x_batch, y_batch))\n",
    "    self.opt.zero_grad()\n",
    "\n",
    "    with training_mode(self.model, is_train=is_train):\n",
    "        logits = self.model(x_batch)\n",
    "\n",
    "    main_loss = self.loss_function(logits, y_batch).mean()\n",
    "\n",
    "    with training_mode(self.model, is_train=False):\n",
    "        model_edited, success, editability_loss, complexity = self.model.edit(x_edit, y_edit, **kwargs)\n",
    "        logits_updated = model_edited(x_batch)\n",
    "\n",
    "    stability_loss = - (F.softmax(logits.detach(), dim=1) * F.log_softmax(logits_updated, dim=1)).sum(dim=1).mean()\n",
    "\n",
    "    final_loss = main_loss + self.stability_coeff * stability_loss + self.editability_coeff * editability_loss\n",
    "\n",
    "    metrics = dict(\n",
    "        final_loss=final_loss.item(), stability_loss=stability_loss.item(),\n",
    "        editability_loss=editability_loss.item(), main_loss=main_loss.item(),\n",
    "    )\n",
    "\n",
    "    final_loss.backward()\n",
    "\n",
    "    if self.max_norm is not None:\n",
    "        metrics['grad_norm'] = torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=self.max_norm)\n",
    "    self.opt.step()\n",
    "\n",
    "    return self.record(**metrics, prefix=prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "roman-vacuum",
   "metadata": {},
   "source": [
    "Note stability loss:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "right-austria",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.4916e-18, device='cuda:0', grad_fn=<NegBackward>)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-(F.softmax(model_out2.logits.detach(), dim=1) * F.log_softmax(model_out1.logits, dim=1)).sum(dim=1).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "phantom-particular",
   "metadata": {},
   "source": [
    "Might need to change dimensions though:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "operational-acquisition",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(6.2890, device='cuda:0', grad_fn=<NegBackward>)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-(F.softmax(model_out2.logits.detach(), dim=-1) * F.log_softmax(model_out1.logits, dim=-1)).sum(dim=-1).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "respective-handling",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 200, 50257])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.log_softmax(model_out1.logits, dim=1).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "oriented-fraud",
   "metadata": {},
   "source": [
    "Conclusions:\n",
    "* Use different stability loss\n",
    "* Double check on softmax dimensions in KL div\n",
    "* Take original model logits in training mode\n",
    "* Performs edit function on model in eval mode (`with training_mode(self.model, is_train=False):`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "enormous-landscape",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
