{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "amazing-greek",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import higher\n",
    "\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "\n",
    "import utils"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "swedish-button",
   "metadata": {},
   "source": [
    "## Load Model, Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "composed-oracle",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "everyday-denmark",
   "metadata": {},
   "source": [
    "Try using GPT2 vs distilgpt2 (KL div actually goes up though)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "harmful-texas",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadOTSModel():\n",
    "    model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "hollow-knock",
   "metadata": {},
   "outputs": [],
   "source": [
    "model, tokenizer = loadOTSModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "empirical-feelings",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = utils.retrieveDataloader(\n",
    "        tokenizer, \n",
    "        bs=1, \n",
    "        dataset='train'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "acoustic-calculation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20892"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "pleased-spirit",
   "metadata": {},
   "outputs": [],
   "source": [
    "for train_step, (lm_data, edit_example, _) in enumerate(dataloader):\n",
    "\n",
    "    lm_tokens, lm_mask = lm_data\n",
    "    lm_tokens, lm_mask = lm_tokens.to(device), lm_mask.to(device)\n",
    "    edit_tokens, edit_mask = edit_example\n",
    "    edit_tokens, edit_mask = edit_tokens.to(device), edit_mask.to(device)\n",
    "\n",
    "    lm_labels = lm_tokens.masked_fill(lm_mask == 0, -100)\n",
    "    edit_labels = edit_tokens.masked_fill(edit_mask == 0, -100) \n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alone-connection",
   "metadata": {},
   "source": [
    "## Double Check Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "emerging-bradley",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[14291,   465, 12928,   422, 29032,  3841,   837, 18322,  4488,   329,\n",
       "         39964,  1578,   764,  2102,   837,  1708,   281,  5095,   287,   662,\n",
       "          2488,    12,    31,  1622,   837, 18322,  1043,  6443,  3614,   837,\n",
       "           878,  4191,  1642,  1478,  4652, 11057,   764,   679,  4191,  1364,\n",
       "           262,  3430,  1863,   351,  5891, 26318,  4186,  6706,   626,   735,\n",
       "           287,  3269,  1853,   764,   198, 50256, 50256, 50256, 50256, 50256,\n",
       "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "flying-array",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0]], device='cuda:0')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "liberal-engineering",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[14291,   465, 12928,   422, 29032,  3841,   837, 18322,  4488,   329,\n",
       "         39964,  1578,   764,  2102,   837,  1708,   281,  5095,   287,   662,\n",
       "          2488,    12,    31,  1622,   837, 18322,  1043,  6443,  3614,   837,\n",
       "           878,  4191,  1642,  1478,  4652, 11057,   764,   679,  4191,  1364,\n",
       "           262,  3430,  1863,   351,  5891, 26318,  4186,  6706,   626,   735,\n",
       "           287,  3269,  1853,   764,   198,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "unsigned-thing",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Following his departure from Dartford, Julian signed for Sutton United. However, following an injury in pre @-@ season, Julian found opportunities limited, before eventually making 14 league appearances. He eventually left the club along with fellow goalkeeper Tom Lovelock in January 2015.\\n'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(lm_tokens[lm_labels != -100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "sized-canon",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Following his departure from Dartford, Julian signed for Sutton United. However, following an injury in pre @-@ season, Julian found opportunities limited, before eventually making 14 league appearances. He eventually left the club along with fellow goalkeeper Tom Lovelock in January 2015.\\n'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(lm_labels[lm_labels != -100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "frank-fiction",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(lm_tokens[lm_labels != -100]) == tokenizer.decode(lm_labels[lm_labels != -100])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "square-abuse",
   "metadata": {},
   "source": [
    "## KL Divergence on same model, same data, model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "satisfied-rates",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train()\n",
    "model.to(device)\n",
    "model_out1 = model(lm_tokens, attention_mask=lm_mask, labels=lm_labels)\n",
    "model_out2 = model(lm_tokens, attention_mask=lm_mask, labels=lm_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "synthetic-snapshot",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(4.7378, device='cuda:0', grad_fn=<NllLossBackward>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_out1.loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "known-password",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(4.7853, device='cuda:0', grad_fn=<NllLossBackward>)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_out2.loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "binding-pepper",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ -52.1889,  -49.2521,  -53.2464,  ...,  -57.0591,  -57.8064,\n",
       "           -51.0275],\n",
       "         [-114.7670, -112.6983, -116.3967,  ..., -116.5385, -118.8621,\n",
       "          -114.7493],\n",
       "         [ -80.4369,  -80.6769,  -84.7900,  ...,  -91.0321,  -89.2350,\n",
       "           -80.6519],\n",
       "         ...,\n",
       "         [ -87.1812,  -81.2851,  -83.2767,  ..., -100.2840, -100.6888,\n",
       "           -87.7263],\n",
       "         [-100.7187,  -93.6546,  -96.0697,  ..., -114.3333, -115.1156,\n",
       "          -100.3458],\n",
       "         [ -99.4097,  -92.7169,  -94.6740,  ..., -112.7470, -113.1586,\n",
       "           -99.6057]]], device='cuda:0', grad_fn=<UnsafeViewBackward>)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_out1.logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "surprising-processor",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 51.0846,  53.7476,  52.9366,  ...,  51.1884,  52.1163,  51.8332],\n",
       "         [  5.0974,   5.1479,   5.0495,  ...,   2.5277,   4.3919,   4.6661],\n",
       "         [  5.8234,   5.0289,   6.1525,  ...,   3.7514,   4.2042,   4.9342],\n",
       "         ...,\n",
       "         [ 14.2161,  13.4051,  13.5858,  ...,  14.9314,  14.3592,  14.2416],\n",
       "         [-24.7716, -23.9950, -24.3571,  ..., -25.1900, -25.8274, -24.3863],\n",
       "         [ -7.7332,  -7.5327,  -7.2533,  ...,  -6.8947,  -6.8626,  -7.0554]]],\n",
       "       device='cuda:0', grad_fn=<SubBackward0>)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_out1.logits - model_out2.logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "infrared-scroll",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 200, 50257])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_out1.logits.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "changed-librarian",
   "metadata": {},
   "source": [
    "This seems like the wrong dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "concerned-deposit",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1.,  ..., 1., 1., 1.]], device='cuda:0',\n",
       "       grad_fn=<SumBackward1>)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum(F.softmax(model_out1.logits, dim=1), dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "critical-waste",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 50257])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum(F.softmax(model_out1.logits, dim=1), dim=1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "romance-windows",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(300279.2812, device='cuda:0', grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.kl_div(\n",
    "    F.log_softmax(model_out1.logits, dim=1),\n",
    "    F.log_softmax(model_out2.logits, dim=1),\n",
    "    reduction='batchmean',\n",
    "    log_target=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "powered-reverse",
   "metadata": {},
   "source": [
    "This seems like the right dimension - softmax over the 50257 words in vocab for each position in sequence 1-200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "later-monroe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000]], device='cuda:0', grad_fn=<SumBackward1>)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum(F.softmax(model_out1.logits, dim=-1), dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "suited-pricing",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 200])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum(F.softmax(model_out1.logits, dim=-1), dim=-1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "lovely-specification",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(35.5365, device='cuda:0', grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.kl_div(\n",
    "    F.log_softmax(model_out1.logits, dim=-1),\n",
    "    F.log_softmax(model_out2.logits, dim=-1),\n",
    "    reduction='batchmean',\n",
    "    log_target=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "large-boundary",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(35.5365, device='cuda:0', grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kl_loss = nn.KLDivLoss(reduction = 'batchmean')\n",
    "l_loc = kl_loss(\n",
    "    F.log_softmax(model_out1.logits, dim=-1),\n",
    "    F.softmax(model_out2.logits, dim=-1)\n",
    ")\n",
    "l_loc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "derived-syntax",
   "metadata": {},
   "source": [
    "## KL Divergence on same model, same data, model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "coordinated-carol",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0., device='cuda:0', grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()\n",
    "model_eval1 = model(lm_tokens, attention_mask=lm_mask, labels=lm_labels)\n",
    "model_eval2 = model(lm_tokens, attention_mask=lm_mask, labels=lm_labels)\n",
    "F.kl_div(\n",
    "    F.log_softmax(model_eval1.logits, dim=-1),\n",
    "    F.log_softmax(model_eval2.logits, dim=-1),\n",
    "    reduction='batchmean',\n",
    "    log_target=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "friendly-nutrition",
   "metadata": {},
   "source": [
    "Large KL divergence coming from dropout?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "invisible-poster",
   "metadata": {},
   "source": [
    "## From editable code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "animal-defendant",
   "metadata": {},
   "outputs": [],
   "source": [
    "def edit(self, inputs, targets, max_steps=None, model_kwargs=None, loss_kwargs=None, opt_kwargs=None, **kwargs):\n",
    "    \"\"\"\n",
    "    Attempts to edit model (out-of-place) and return an edited copy\n",
    "    :param inputs: data that is fed into the model\n",
    "    :param targets: reference answers that are fed into loss function\n",
    "    :param max_steps: after this many gradient steps the process is terminated\n",
    "    :param model_kwargs: optional extra model inputs, used as model(inputs, **model_params)\n",
    "    :param loss_kwargs: optional extra loss parameters, self.loss_function(model(inputs), targets, **loss_params)\n",
    "    :param opt_kwargs: optional overrides for optimizer.get_initial_state\n",
    "    :param kwargs: extra parameters passed to optimizer.step\n",
    "    :returns: edited_model, is_edit_successful, final_loss, gradients_steps\n",
    "    :rtype: Editable.EditResult\n",
    "    \"\"\"\n",
    "    model_kwargs, loss_kwargs, opt_kwargs = model_kwargs or {}, loss_kwargs or {}, opt_kwargs or {}\n",
    "    optimizer_state = self.optimizer.get_initial_state(self, **opt_kwargs)\n",
    "    editable = self\n",
    "\n",
    "    for step in count():\n",
    "        prediction = editable(inputs, **model_kwargs)\n",
    "        loss = self.loss_function(prediction, targets, **loss_kwargs)\n",
    "\n",
    "        if self.is_edit_finished(**locals()):\n",
    "            return self.EditResult(editable, success=True, loss=loss, complexity=step)\n",
    "        elif step >= (max_steps or self.max_steps):\n",
    "            return self.EditResult(editable, success=False, loss=loss, complexity=step)\n",
    "\n",
    "        optimizer_state, editable = self.optimizer.step(\n",
    "            optimizer_state, editable, loss, parameters=editable.get_editable_parameters(editable.module), **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "comic-amino",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_on_batch(self, x_batch, y_batch, x_edit, y_edit, prefix='train/', is_train=True, **kwargs):\n",
    "    \"\"\" Performs a single gradient update and reports metrics \"\"\"\n",
    "    x_batch, y_batch = map(torch.as_tensor, (x_batch, y_batch))\n",
    "    self.opt.zero_grad()\n",
    "\n",
    "    with training_mode(self.model, is_train=is_train):\n",
    "        logits = self.model(x_batch)\n",
    "\n",
    "    main_loss = self.loss_function(logits, y_batch).mean()\n",
    "\n",
    "    with training_mode(self.model, is_train=False):\n",
    "        model_edited, success, editability_loss, complexity = self.model.edit(x_edit, y_edit, **kwargs)\n",
    "        logits_updated = model_edited(x_batch)\n",
    "\n",
    "    stability_loss = - (F.softmax(logits.detach(), dim=1) * F.log_softmax(logits_updated, dim=1)).sum(dim=1).mean()\n",
    "\n",
    "    final_loss = main_loss + self.stability_coeff * stability_loss + self.editability_coeff * editability_loss\n",
    "\n",
    "    metrics = dict(\n",
    "        final_loss=final_loss.item(), stability_loss=stability_loss.item(),\n",
    "        editability_loss=editability_loss.item(), main_loss=main_loss.item(),\n",
    "    )\n",
    "\n",
    "    final_loss.backward()\n",
    "\n",
    "    if self.max_norm is not None:\n",
    "        metrics['grad_norm'] = torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=self.max_norm)\n",
    "    self.opt.step()\n",
    "\n",
    "    return self.record(**metrics, prefix=prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "planned-nightmare",
   "metadata": {},
   "source": [
    "Note stability loss:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "european-rolling",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(6.3995, device='cuda:0', grad_fn=<NegBackward>)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-(F.softmax(model_out2.logits.detach(), dim=1) * F.log_softmax(model_out1.logits, dim=1)).sum(dim=1).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "noticed-crash",
   "metadata": {},
   "source": [
    "Might need to change dimensions though:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "tracked-antigua",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(6.0217, device='cuda:0', grad_fn=<NegBackward>)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-(F.softmax(model_out2.logits.detach(), dim=-1) * F.log_softmax(model_out1.logits, dim=-1)).sum(dim=-1).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "hungarian-harbor",
   "metadata": {},
   "outputs": [],
   "source": [
    "P = F.log_softmax(model_out2.logits.detach(), dim=-1) \n",
    "Q = F.log_softmax(model_out1.logits, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "distant-friend",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(35.5365, device='cuda:0', grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(P * (P / Q).log()).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "floral-property",
   "metadata": {},
   "outputs": [],
   "source": [
    "stability_loss = (\n",
    "    F.softmax(base_out.logits.detach(), dim=-1)\n",
    "   * (F.log_softmax(base_out.logits.detach(), dim=-1) - F.log_softmax(edited_base_out.logits, dim=-1))\n",
    "     ).sum(-1).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "sharp-lender",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 200, 50257])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(P * (P / Q).log()).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "conceptual-vietnam",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.1777, device='cuda:0', grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(P * (P / Q).log()).sum(-1).mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "direct-attribute",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/py37_pytorch/lib/python3.7/site-packages/torch/nn/functional.py:2398: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.\n",
      "  warnings.warn(\"reduction: 'mean' divides the total loss by both the batch size and the support size.\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(3.5355e-06, device='cuda:0', grad_fn=<KlDivBackward>)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.kl_div(\n",
    "    F.log_softmax(model_out1.logits, dim=-1),\n",
    "    F.log_softmax(model_out2.logits, dim=-1),\n",
    "    reduction='mean',\n",
    "    log_target=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "increasing-atlantic",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(6.0217, device='cuda:0', grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(P * -Q.log()).sum(dim=-1).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "contrary-round",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(35.5365, device='cuda:0', grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "# F.kl_div(Q.log(), P, None, None, 'sum')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "transparent-ballet",
   "metadata": {},
   "source": [
    "Conclusions:\n",
    "* Use different stability loss\n",
    "* Double check on softmax dimensions in KL div\n",
    "* Take original model logits in training mode\n",
    "* Performs edit function on model in eval mode (`with training_mode(self.model, is_train=False):`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "printable-terrorism",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
